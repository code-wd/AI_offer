> **主成分分析(Principle Components Analysis ,PCA)**和**线性评判分析（Linear Discriminant Analysis,LDA）**是特征抽取的两种主要经典方法

- 特征提取有两种类型：
  1. **Signal representation(信号表示):** 特征抽取后的特征要能够精确地表示样本信息，使得信息丢失很小（The goal of the feature extraction mapping is to represent the samples accurately in a low-dimensional space.）对应的方法是PCA
  2. **Signal classification（信号分类):** 特征抽取后的特征，要使得分类后的准确率很高，不能比原来特征进行分类的准确率低（The goal of the feature extraction mapping is toenhance the class-discriminatory information in a low-dimensional space.）对与线性来说，对应的方法是LDA . 非线性暂时不考虑
- 不同点：
  - PCA得到的投影空间是协方差矩阵的特征向量
  - 而 LDA 则是通过求得一个变换 W，使得变换之后的新均值之差最大、方差最大（也就是最大化类间距离和最小化类内距离），变换 W 就是特征的投影方向

# 一、主成分分析（PCA）

> PCA 是降维最经典的方法，它旨在是找到**数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的**。
>
> PCA 的思想是**通过坐标轴转换，寻找数据分布的最优子空间**

**PCA是最常用的线性降维方法**，**它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大（样本的分布最散乱）以使用较少的数据维度同时保留住较多的原数据点的特征**。

- 举例：
  - 在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度
  - 但实际上这些数据点都在同一个二维平面上
  - 如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y' 轴来表示原始数据，
  - 并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分

## 1）求解步骤

假设有 m 条 n 维数据：

1. 中心化
   1. 将原始数据按列组成 n 行 m 列的矩阵 X
   2. 将 X 的每一行中心化处理，即减去这一行的均值
2. 求出协方差矩阵 $C=\frac{1}{m}XX^T$
3. 求出协方差矩阵的特征值以及对应的特征向量
4. 将特征向量按对应特征值大小，从大到小排列成从上到下的矩阵，取前 k 行组成矩阵 P
5. Y=PX 即是降维到 k 维后的数据

- 解释：
  - 通过 PCA ，就可以将方差较小的特征给抛弃
  - 这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差
  - **特征值越大，方差越大，信息量也就越大**。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。

## 2）优缺点

### 2.1 优点

1. 缓解维度灾难：PCA 算法通过舍去一部分信息，能使得样本的采样密度增大（因为维度降低），这是缓解维度灾难的重要手段
2. 降噪：当数据收到噪声影响的时候，最小特征值对应的特征向量往往与噪声有关，将这些小特征值对应的特征销量舍弃，可以起到降噪的效果
3. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响
4. 特征独立：各主成分之间正交，可消除原始数据成分间的相互影响的因素，使得数据各个特征相互独立
5. 计算方法简单，主要运算是特征值分解，易于实现

### 2.2 缺点

1. 可能加剧过拟合：PCA 保留了主要信息，但是主要信息只是针对训练集，而且这个主要信息可能并不是重要信息，有可能舍弃了一些不是主要信息但是非常重要的信息，从而加剧过拟合
2. 可解释性降低：提取出的各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强

- 解决 PCA 方法的一些缺点：
  - 为解决非线性降维的KPCA
  - 解决内存限制的增量PCA方法Incremental PCA
  - 解决稀疏数据降维的PCA方法Sparse PCA

# 二、线性判别分析法（LDA）

> LDA 是一种**有监督学习算法**，相比较 PCA，它**考虑到数据的类别信息**，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上
>
> 因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个**可以最大化类间距离，以及最小化类内距离**的方向。

## 1）LDA 优缺点

### 优点

1. 相比较 PCA，LDA 更加擅长处理**带有类别信息的数据，**因为可以使用类别的先验知识经验
2. 线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法
3. LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优

### 缺点

1. LDA 对数据的分布做出了很强的假设

   - 比如每个类别数据都是高斯分布、各个类的协方差相等，这些假设在实际中不一定完全满足，因此不适合对非高斯分布样本进行降维

2. LDA 模型简单，表达能力有一定局限性

   - 但这可以通过引入**核函数**拓展 LDA 来处理分布比较复杂的数据

3. LDA可能过度拟合数据

4. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好

# Reference

1. [特征工程(完）](https://zhuanlan.zhihu.com/p/57356973)【可以查看图像、文本特征提取】
2. [特征选择与特征抽取](https://cloud.tencent.com/developer/article/1411702)
3. [机器学习之数据清洗、特征提取与特征选择](https://www.toutiao.com/i6531647729829937668/?wid=1635671319665)
4. [机器学习之特征选择和特征抽取](https://www.cnblogs.com/dyl222/p/11055756.html)
5. [机器学习之数据清洗、特征提取与特征选择](https://www.toutiao.com/i6531647729829937668/?wid=1635671319665)

